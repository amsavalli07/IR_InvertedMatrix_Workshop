{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyzZ1Ib2CM3Z",
        "outputId": "3c89bde1-898e-41ec-d216-16d2cf35daf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.7.9)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Document Collection\n",
        "\n",
        "In this step, we will simulate a small document corpus (text dataset) consisting of a few short text samples. These documents will be used to build our Inverted Index.\n",
        "\n",
        "**Talking Point 1:** A good test document collection should contain varied vocabulary and overlapping terms to verify token mapping accuracy in the inverted index.\n",
        "\n",
        "**Talking Point 2:** For real-world use, text may come from files, web scraping, or databases ‚Äî but we‚Äôll keep it simple here using plain strings in memory.\n"
      ],
      "metadata": {
        "id": "QjkBwMCJCtcg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import random\n",
        "\n",
        "# Load a subset of the dataset for quick processing\n",
        "newsgroups_data = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# We'll take only the first 20 documents for this workshop\n",
        "documents = newsgroups_data.data[:20]\n",
        "\n",
        "# Assign unique IDs to each document\n",
        "document_dict = {f'doc_{i}': doc for i, doc in enumerate(documents)}\n",
        "\n",
        "# Preview first document\n",
        "print(f\"Sample doc ID: doc_0\\n\\n{document_dict['doc_0'][:500]}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAe8u7R8C0iP",
        "outputId": "355d267f-aa25-45f5-aab1-b7c1ede6f462"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample doc ID: doc_0\n",
            "\n",
            "I was wondering if anyone out there could enlighten me on this car I saw\n",
            "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
            "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
            "the front bumper was separate from the rest of the body. This is \n",
            "all I know. If anyone can tellme a model name, engine specs, years\n",
            "of production, where this car is made, history, or whatever info you\n",
            "have on this funky looking car, please e-mail....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 2: Tokenizer Implementation\n",
        "\n",
        "Talking Point 2 (Markdown):\n",
        "A tokenizer breaks raw text into words. A custom tokenizer gives us flexibility to handle domain-specific patterns, punctuation, and edge cases. For example, we might want to keep hyphenated words or handle contractions differently.\n"
      ],
      "metadata": {
        "id": "xqisIaRJDMum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    Tokenizes the input text into words.\n",
        "    - Lowercases\n",
        "    - Removes punctuation\n",
        "    - Splits on whitespace\n",
        "    \"\"\"\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation using regex\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Split into tokens\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "# Example usage on one document\n",
        "sample_tokens = tokenize(document_dict['doc_0'])\n",
        "print(sample_tokens[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nC0RrjriDPYN",
        "outputId": "2b821cda-2390-48de-e808-393e82db8500"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'i', 'saw', 'the', 'other', 'day', 'it', 'was']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Normalization Pipeline\n",
        " Text normalization (like removing stop words and stemming) reduces noise and improves IR accuracy. For example, \"running\", \"runs\", and \"ran\" all reduce to \"run\" ‚Äî helping the Inverted Index map them to the same concept."
      ],
      "metadata": {
        "id": "k32eU5S-DrcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize stopwords and stemmer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def normalize(tokens):\n",
        "    \"\"\"\n",
        "    Normalizes a list of tokens by:\n",
        "    - Removing stop words\n",
        "    - Applying stemming\n",
        "    \"\"\"\n",
        "    normalized = []\n",
        "    for token in tokens:\n",
        "        if token not in stop_words:\n",
        "            stemmed = stemmer.stem(token)\n",
        "            normalized.append(stemmed)\n",
        "    return normalized\n",
        "\n",
        "# Example: Normalize a sample document\n",
        "tokens = tokenize(document_dict['doc_0'])\n",
        "normalized_tokens = normalize(tokens)\n",
        "\n",
        "print(normalized_tokens[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9SLFgUmD-ca",
        "outputId": "9def9275-0457-42ce-ec84-718c20b0b897"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['wonder', 'anyon', 'could', 'enlighten', 'car', 'saw', 'day', '2door', 'sport', 'car', 'look', 'late', '60', 'earli', '70', 'call', 'bricklin', 'door', 'realli', 'small']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Build and Query the Inverted Index\n",
        "An Inverted Index maps each term to a list of documents it appears in. This makes search extremely efficient, especially at scale. It's the backbone of search engines and information retrieval systems."
      ],
      "metadata": {
        "id": "b2s7ByqwECGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Build the inverted Index\n",
        "inverted_index = {\n",
        "    \"term1\": {\"doc_3\", \"doc_7\"},\n",
        "    \"term2\": {\"doc_1\", \"doc_4\", \"doc_9\"},\n",
        "}"
      ],
      "metadata": {
        "id": "7d8irDoKENuP"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def build_inverted_index(documents):\n",
        "    \"\"\"\n",
        "    Builds an inverted index from the input dictionary of documents.\n",
        "    \"\"\"\n",
        "    index = defaultdict(set)\n",
        "\n",
        "    for doc_id, text in documents.items():\n",
        "        tokens = tokenize(text)\n",
        "        normalized_tokens = normalize(tokens)\n",
        "\n",
        "        for token in normalized_tokens:\n",
        "            index[token].add(doc_id)\n",
        "\n",
        "    return index\n",
        "\n",
        "# Build and store the inverted index\n",
        "inverted_index = build_inverted_index(document_dict)\n",
        "\n",
        "# Preview sample entries\n",
        "for i, (term, doc_ids) in enumerate(inverted_index.items()):\n",
        "    print(f\"{term}: {sorted(list(doc_ids))}\")\n",
        "    if i == 9:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBEXZPjQEYqT",
        "outputId": "eaccbf69-89c9-40fb-9a64-98af30ec4cd8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wonder: ['doc_0', 'doc_2']\n",
            "anyon: ['doc_0', 'doc_16', 'doc_18']\n",
            "could: ['doc_0', 'doc_11', 'doc_17', 'doc_2']\n",
            "enlighten: ['doc_0']\n",
            "car: ['doc_0', 'doc_17']\n",
            "saw: ['doc_0']\n",
            "day: ['doc_0', 'doc_1', 'doc_13']\n",
            "2door: ['doc_0']\n",
            "sport: ['doc_0', 'doc_17']\n",
            "look: ['doc_0', 'doc_13', 'doc_15', 'doc_2']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phrase Query Function\n",
        "We'll implement a basic query function that:\n",
        "\n",
        "Normalizes input query\n",
        "\n",
        "Finds documents containing all tokens (AND operation)"
      ],
      "metadata": {
        "id": "H_RXzQYNEZiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search(query, index):\n",
        "    \"\"\"\n",
        "    Search for documents that contain **all words** in the query.\n",
        "    \"\"\"\n",
        "    query_tokens = normalize(tokenize(query))\n",
        "\n",
        "    if not query_tokens:\n",
        "        return []\n",
        "\n",
        "    # Initialize result with docs containing the first token\n",
        "    result_docs = index.get(query_tokens[0], set()).copy()\n",
        "\n",
        "    # Intersect with other tokens' doc sets\n",
        "    for token in query_tokens[1:]:\n",
        "        result_docs &= index.get(token, set())\n",
        "\n",
        "    return sorted(result_docs)\n",
        "\n",
        "# üîç Run 2 Phrase Queries\n",
        "query1 = \"computer science\"\n",
        "query2 = \"space nasa mission\"\n",
        "\n",
        "print(f\"\\nResults for '{query1}': {search(query1, inverted_index)}\")\n",
        "print(f\"Results for '{query2}': {search(query2, inverted_index)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfuhC6eSIx4z",
        "outputId": "d7e4ddd3-ead5-4b29-9ad1-3fc9f824c337"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Results for 'computer science': []\n",
            "Results for 'space nasa mission': []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Query 1 normalized:\", normalize(tokenize(\"computer science\")))\n",
        "print(\"Query 2 normalized:\", normalize(tokenize(\"space nasa mission\")))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45lVj0_sFVpD",
        "outputId": "6c4fa9ef-aa38-4d82-8a56-fc0dfb962008"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query 1 normalized: ['comput', 'scienc']\n",
            "Query 2 normalized: ['space', 'nasa', 'mission']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Query 1: 'comput', 'scienc'\n",
        "print(\"Docs with 'comput':\", inverted_index.get('comput', []))\n",
        "print(\"Docs with 'scienc':\", inverted_index.get('scienc', []))\n",
        "\n",
        "# Query 2: 'space', 'nasa', 'mission'\n",
        "print(\"Docs with 'space':\", inverted_index.get('space', []))\n",
        "print(\"Docs with 'nasa':\", inverted_index.get('nasa', []))\n",
        "print(\"Docs with 'mission':\", inverted_index.get('mission', []))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbsJF4SXGw--",
        "outputId": "eec10d0f-753f-421b-9cd1-1ec695ea1d1d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Docs with 'comput': {'doc_13', 'doc_2'}\n",
            "Docs with 'scienc': []\n",
            "Docs with 'space': {'doc_13'}\n",
            "Docs with 'nasa': []\n",
            "Docs with 'mission': {'doc_13'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def search_or(query, index):\n",
        "    query_tokens = normalize(tokenize(query))\n",
        "    result_docs = set()\n",
        "    for token in query_tokens:\n",
        "        result_docs |= index.get(token, set())\n",
        "    return sorted(result_docs)\n",
        "\n",
        "print(\"OR Search - computer science:\", search_or(\"computer science\", inverted_index))\n",
        "print(\"OR Search - space nasa mission:\", search_or(\"space nasa mission\", inverted_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6_vV8GiHcnV",
        "outputId": "ba8aa830-05b5-491f-b589-d00172cc809a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OR Search - computer science: ['doc_13', 'doc_2']\n",
            "OR Search - space nasa mission: ['doc_13']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phrase Query Tests\n",
        "\n",
        "*Talking Point 5:* Phrase queries using AND logic are strict ‚Äî all terms must exist in the same document. If even one is missing, the query returns no result. This highlights how token coverage impacts IR accuracy.\n",
        "```python\n",
        "# AND Queries\n",
        "print(\"Search (AND) - 'space mission':\", search(\"space mission\", inverted_index))\n",
        "print(\"Search (AND) - 'comput mission':\", search(\"comput mission\", inverted_index))\n",
        "\n",
        "# OR Queries\n",
        "print(\"Search (OR) - 'computer science':\", search_or(\"computer science\", inverted_index))\n",
        "print(\"Search (OR) - 'space nasa mission':\", search_or(\"space nasa mission\", inverted_index))"
      ],
      "metadata": {
        "id": "aw8xWi0CH0r4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Stemming ‚Äì Applied PorterStemmer\n",
        "Add Stemming Using NLTK‚Äôs PorterStemmer\n",
        "Here‚Äôs exactly what to add to your existing normalize() function"
      ],
      "metadata": {
        "id": "X3yJ0d4UPJX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "# Download required resources\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize stemmer and stopword list\n",
        "stemmer = PorterStemmer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Normalization function\n",
        "def normalize(tokens):\n",
        "    normalized = []\n",
        "    for token in tokens:\n",
        "        token = token.lower().strip(string.punctuation)\n",
        "        if token and token not in stop_words:\n",
        "            stemmed = stemmer.stem(token)\n",
        "            normalized.append(stemmed)\n",
        "    return normalized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZjA3nMyPDwl",
        "outputId": "f9186324-6b20-4fb5-8213-ac2f215c3072"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizer_for_vectorizer(text):\n",
        "    return normalize(tokenize(text))"
      ],
      "metadata": {
        "id": "wVsbb_YTPzUV"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [\"computers\", \"running\", \"happily\", \"drives\", \"mice\"]\n",
        "print(\"After stemming:\", normalize(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yja9R8pPP2up",
        "outputId": "faed5b29-33dd-4a05-afb7-d82ff7312e03"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After stemming: ['comput', 'run', 'happili', 'drive', 'mice']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Vectorization and Cosine Similarity\n",
        "Vectorization converts processed text into numerical form so we can compute similarity between documents. We use TF-IDF to give higher weight to rare but meaningful terms. Cosine similarity helps identify how similar two documents are in terms of their content, regardless of their length.\n"
      ],
      "metadata": {
        "id": "SnaP31fhLowF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Reuse the same documents (we'll only use 5 here for quick testing)\n",
        "sample_docs = [document_dict[f'doc_{i}'] for i in range(5)]\n",
        "\n",
        "# Initialize vectorizer with preprocessing\n",
        "vectorizer = TfidfVectorizer(\n",
        "    tokenizer=tokenize,         # Custom tokenizer\n",
        "    stop_words='english',       # Built-in stopword removal\n",
        "    lowercase=True              # Lowercasing\n",
        ")\n",
        "\n",
        "# Transform documents into TF-IDF matrix\n",
        "tfidf_matrix = vectorizer.fit_transform(sample_docs)\n",
        "\n",
        "# Compute cosine similarity matrix (doc-to-doc)\n",
        "cosine_sim_matrix = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "# Display similarity between doc_0 and others\n",
        "print(\"Cosine Similarity between doc_0 and other docs:\")\n",
        "for i in range(len(sample_docs)):\n",
        "    print(f\"doc_0 vs doc_{i}: {cosine_sim_matrix[0][i]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFvlPihBL5DL",
        "outputId": "773e4b59-d854-45e4-92bb-ae10a71013d0"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity between doc_0 and other docs:\n",
            "doc_0 vs doc_0: 1.0000\n",
            "doc_0 vs doc_1: 0.0124\n",
            "doc_0 vs doc_2: 0.0538\n",
            "doc_0 vs doc_3: 0.0000\n",
            "doc_0 vs doc_4: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"TF-IDF Vocabulary:\", list(vectorizer.vocabulary_.keys()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-FaI-PrOVlc",
        "outputId": "a7ff66df-a7a0-4c5c-e129-87b6a4560dc1"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Vocabulary: ['wondering', 'enlighten', 'car', 'saw', 'day', '2door', 'sports', 'looked', 'late', '60s', 'early', '70s', 'called', 'bricklin', 'doors', 'really', 'small', 'addition', 'bumper', 'separate', 'rest', 'body', 'know', 'tellme', 'model', 'engine', 'specs', 'years', 'production', 'history', 'info', 'funky', 'looking', 'email', 'fair', 'number', 'brave', 'souls', 'upgraded', 'si', 'clock', 'oscillator', 'shared', 'experiences', 'poll', 'send', 'brief', 'message', 'detailing', 'procedure', 'speed', 'attained', 'cpu', 'rated', 'add', 'cards', 'adapters', 'heat', 'sinks', 'hour', 'usage', 'floppy', 'disk', 'functionality', '800', '14', 'm', 'floppies', 'especially', 'requested', 'summarizing', 'days', 'network', 'knowledge', 'base', 'upgrade', 'havent', 'answered', 'thanks', 'folks', 'mac', 'plus', 'finally', 'gave', 'ghost', 'weekend', 'starting', 'life', '512k', 'way', '1985', 'sooo', 'im', 'market', 'new', 'machine', 'bit', 'sooner', 'intended', 'picking', 'powerbook', '160', 'maybe', '180', 'bunch', 'questions', 'hopefully', 'somebody', 'answer', 'does', 'anybody', 'dirt', 'round', 'introductions', 'expected', 'id', 'heard', '185c', 'supposed', 'make', 'appearence', 'summer', 'anymore', 'dont', 'access', 'macleak', 'rumors', 'price', 'drops', 'line', 'like', 'ones', 'duos', 'just', 'went', 'recently', 'whats', 'impression', 'display', 'probably', 'swing', 'got', '80mb', '120', 'feel', 'better', 'yea', 'looks', 'great', 'store', 'wow', 'good', 'solicit', 'opinions', 'people', 'use', 'daytoday', 'worth', 'taking', 'size', 'money', 'hit', 'active', 'realize', 'real', 'subjective', 'question', 'ive', 'played', 'machines', 'computer', 'breifly', 'figured', 'actually', 'uses', 'daily', 'prove', 'helpful', 'hellcats', 'perform', 'advance', 'ill', 'post', 'summary', 'news', 'reading', 'time', 'premium', 'finals', 'corner', 'tom', 'willis', 'twillisecnpurdueedu', 'purdue', 'electrical', 'engineering', 'weiteks', 'addressphone', 'information', 'chip', 'article', 'c5owcbn3pworldstdcom', 'tombakerworldstdcom', 'baker', 'understanding', 'errors', 'basically', 'known', 'bugs', 'warning', 'software', 'things', 'checked', 'right', 'values', 'arent', 'set', 'till', 'launch', 'suchlike', 'fix', 'code', 'possibly', 'introduce', 'tell', 'crew', 'ok', '213', 'liftoff', 'ignore']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query Test (TF-IDF + Cosine):\n",
        "We now test the similarity of a user query like \"space shuttle program\" to each of the documents and rank them."
      ],
      "metadata": {
        "id": "yrQEEuKuMGRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = [\"floppy disk\"]\n",
        "query_vec = vectorizer.transform(query)\n",
        "similarities = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
        "\n",
        "ranked_docs = sorted(zip(range(len(sample_docs)), similarities), key=lambda x: -x[1])\n",
        "\n",
        "print(f\"\\nQuery: '{query[0]}'\")\n",
        "print(\"Ranked documents (doc_id, similarity score):\")\n",
        "for doc_id, score in ranked_docs:\n",
        "    print(f\"doc_{doc_id}: {score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZQvYB_CMQPs",
        "outputId": "24338b06-d4a1-4b1f-c8f9-bd884854cb21"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Query: 'floppy disk'\n",
            "Ranked documents (doc_id, similarity score):\n",
            "doc_1: 0.1669\n",
            "doc_2: 0.0714\n",
            "doc_0: 0.0000\n",
            "doc_3: 0.0000\n",
            "doc_4: 0.0000\n"
          ]
        }
      ]
    }
  ]
}