{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyzZ1Ib2CM3Z",
        "outputId": "3c89bde1-898e-41ec-d216-16d2cf35daf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.7.9)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Document Collection\n",
        "\n",
        "In this step, we will simulate a small document corpus (text dataset) consisting of a few short text samples. These documents will be used to build our Inverted Index.\n",
        "\n",
        "**Talking Point 1:** A good test document collection should contain varied vocabulary and overlapping terms to verify token mapping accuracy in the inverted index.\n",
        "\n",
        "**Talking Point 2:** For real-world use, text may come from files, web scraping, or databases — but we’ll keep it simple here using plain strings in memory.\n"
      ],
      "metadata": {
        "id": "QjkBwMCJCtcg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import random\n",
        "\n",
        "# Load a subset of the dataset for quick processing\n",
        "newsgroups_data = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# We'll take only the first 20 documents for this workshop\n",
        "documents = newsgroups_data.data[:20]\n",
        "\n",
        "# Assign unique IDs to each document\n",
        "document_dict = {f'doc_{i}': doc for i, doc in enumerate(documents)}\n",
        "\n",
        "# Preview first document\n",
        "print(f\"Sample doc ID: doc_0\\n\\n{document_dict['doc_0'][:500]}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAe8u7R8C0iP",
        "outputId": "355d267f-aa25-45f5-aab1-b7c1ede6f462"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample doc ID: doc_0\n",
            "\n",
            "I was wondering if anyone out there could enlighten me on this car I saw\n",
            "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
            "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
            "the front bumper was separate from the rest of the body. This is \n",
            "all I know. If anyone can tellme a model name, engine specs, years\n",
            "of production, where this car is made, history, or whatever info you\n",
            "have on this funky looking car, please e-mail....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 2: Tokenizer Implementation\n",
        "\n",
        "Talking Point 2 (Markdown):\n",
        "A tokenizer breaks raw text into words. A custom tokenizer gives us flexibility to handle domain-specific patterns, punctuation, and edge cases. For example, we might want to keep hyphenated words or handle contractions differently.\n"
      ],
      "metadata": {
        "id": "xqisIaRJDMum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    Tokenizes the input text into words.\n",
        "    - Lowercases\n",
        "    - Removes punctuation\n",
        "    - Splits on whitespace\n",
        "    \"\"\"\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation using regex\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Split into tokens\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "# Example usage on one document\n",
        "sample_tokens = tokenize(document_dict['doc_0'])\n",
        "print(sample_tokens[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nC0RrjriDPYN",
        "outputId": "2b821cda-2390-48de-e808-393e82db8500"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'i', 'saw', 'the', 'other', 'day', 'it', 'was']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Normalization Pipeline\n",
        " Text normalization (like removing stop words and stemming) reduces noise and improves IR accuracy. For example, \"running\", \"runs\", and \"ran\" all reduce to \"run\" — helping the Inverted Index map them to the same concept."
      ],
      "metadata": {
        "id": "k32eU5S-DrcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize stopwords and stemmer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def normalize(tokens):\n",
        "    \"\"\"\n",
        "    Normalizes a list of tokens by:\n",
        "    - Removing stop words\n",
        "    - Applying stemming\n",
        "    \"\"\"\n",
        "    normalized = []\n",
        "    for token in tokens:\n",
        "        if token not in stop_words:\n",
        "            stemmed = stemmer.stem(token)\n",
        "            normalized.append(stemmed)\n",
        "    return normalized\n",
        "\n",
        "# Example: Normalize a sample document\n",
        "tokens = tokenize(document_dict['doc_0'])\n",
        "normalized_tokens = normalize(tokens)\n",
        "\n",
        "print(normalized_tokens[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9SLFgUmD-ca",
        "outputId": "9def9275-0457-42ce-ec84-718c20b0b897"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['wonder', 'anyon', 'could', 'enlighten', 'car', 'saw', 'day', '2door', 'sport', 'car', 'look', 'late', '60', 'earli', '70', 'call', 'bricklin', 'door', 'realli', 'small']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Build and Query the Inverted Index\n",
        "An Inverted Index maps each term to a list of documents it appears in. This makes search extremely efficient, especially at scale. It's the backbone of search engines and information retrieval systems."
      ],
      "metadata": {
        "id": "b2s7ByqwECGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Build the inverted Index\n",
        "inverted_index = {\n",
        "    \"term1\": {\"doc_3\", \"doc_7\"},\n",
        "    \"term2\": {\"doc_1\", \"doc_4\", \"doc_9\"},\n",
        "}"
      ],
      "metadata": {
        "id": "7d8irDoKENuP"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def build_inverted_index(documents):\n",
        "    \"\"\"\n",
        "    Builds an inverted index from the input dictionary of documents.\n",
        "    \"\"\"\n",
        "    index = defaultdict(set)\n",
        "\n",
        "    for doc_id, text in documents.items():\n",
        "        tokens = tokenize(text)\n",
        "        normalized_tokens = normalize(tokens)\n",
        "\n",
        "        for token in normalized_tokens:\n",
        "            index[token].add(doc_id)\n",
        "\n",
        "    return index\n",
        "\n",
        "# Build and store the inverted index\n",
        "inverted_index = build_inverted_index(document_dict)\n",
        "\n",
        "# Preview sample entries\n",
        "for i, (term, doc_ids) in enumerate(inverted_index.items()):\n",
        "    print(f\"{term}: {sorted(list(doc_ids))}\")\n",
        "    if i == 9:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBEXZPjQEYqT",
        "outputId": "eaccbf69-89c9-40fb-9a64-98af30ec4cd8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wonder: ['doc_0', 'doc_2']\n",
            "anyon: ['doc_0', 'doc_16', 'doc_18']\n",
            "could: ['doc_0', 'doc_11', 'doc_17', 'doc_2']\n",
            "enlighten: ['doc_0']\n",
            "car: ['doc_0', 'doc_17']\n",
            "saw: ['doc_0']\n",
            "day: ['doc_0', 'doc_1', 'doc_13']\n",
            "2door: ['doc_0']\n",
            "sport: ['doc_0', 'doc_17']\n",
            "look: ['doc_0', 'doc_13', 'doc_15', 'doc_2']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phrase Query Function\n",
        "We'll implement a basic query function that:\n",
        "\n",
        "Normalizes input query\n",
        "\n",
        "Finds documents containing all tokens (AND operation)"
      ],
      "metadata": {
        "id": "H_RXzQYNEZiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search(query, index):\n",
        "    \"\"\"\n",
        "    Search for documents that contain **all words** in the query.\n",
        "    \"\"\"\n",
        "    query_tokens = normalize(tokenize(query))\n",
        "\n",
        "    if not query_tokens:\n",
        "        return []\n",
        "\n",
        "    # Initialize result with docs containing the first token\n",
        "    result_docs = index.get(query_tokens[0], set()).copy()\n",
        "\n",
        "    # Intersect with other tokens' doc sets\n",
        "    for token in query_tokens[1:]:\n",
        "        result_docs &= index.get(token, set())\n",
        "\n",
        "    return sorted(result_docs)\n",
        "\n",
        "# 🔍 Run 2 Phrase Queries\n",
        "query1 = \"computer science\"\n",
        "query2 = \"space nasa mission\"\n",
        "\n",
        "print(f\"\\nResults for '{query1}': {search(query1, inverted_index)}\")\n",
        "print(f\"Results for '{query2}': {search(query2, inverted_index)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfuhC6eSIx4z",
        "outputId": "d7e4ddd3-ead5-4b29-9ad1-3fc9f824c337"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Results for 'computer science': []\n",
            "Results for 'space nasa mission': []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Query 1 normalized:\", normalize(tokenize(\"computer science\")))\n",
        "print(\"Query 2 normalized:\", normalize(tokenize(\"space nasa mission\")))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45lVj0_sFVpD",
        "outputId": "6c4fa9ef-aa38-4d82-8a56-fc0dfb962008"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query 1 normalized: ['comput', 'scienc']\n",
            "Query 2 normalized: ['space', 'nasa', 'mission']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Query 1: 'comput', 'scienc'\n",
        "print(\"Docs with 'comput':\", inverted_index.get('comput', []))\n",
        "print(\"Docs with 'scienc':\", inverted_index.get('scienc', []))\n",
        "\n",
        "# Query 2: 'space', 'nasa', 'mission'\n",
        "print(\"Docs with 'space':\", inverted_index.get('space', []))\n",
        "print(\"Docs with 'nasa':\", inverted_index.get('nasa', []))\n",
        "print(\"Docs with 'mission':\", inverted_index.get('mission', []))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbsJF4SXGw--",
        "outputId": "eec10d0f-753f-421b-9cd1-1ec695ea1d1d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Docs with 'comput': {'doc_13', 'doc_2'}\n",
            "Docs with 'scienc': []\n",
            "Docs with 'space': {'doc_13'}\n",
            "Docs with 'nasa': []\n",
            "Docs with 'mission': {'doc_13'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def search_or(query, index):\n",
        "    query_tokens = normalize(tokenize(query))\n",
        "    result_docs = set()\n",
        "    for token in query_tokens:\n",
        "        result_docs |= index.get(token, set())\n",
        "    return sorted(result_docs)\n",
        "\n",
        "print(\"OR Search - computer science:\", search_or(\"computer science\", inverted_index))\n",
        "print(\"OR Search - space nasa mission:\", search_or(\"space nasa mission\", inverted_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6_vV8GiHcnV",
        "outputId": "ba8aa830-05b5-491f-b589-d00172cc809a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OR Search - computer science: ['doc_13', 'doc_2']\n",
            "OR Search - space nasa mission: ['doc_13']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phrase Query Tests\n",
        "\n",
        "*Talking Point 5:* Phrase queries using AND logic are strict — all terms must exist in the same document. If even one is missing, the query returns no result. This highlights how token coverage impacts IR accuracy.\n",
        "```python\n",
        "# AND Queries\n",
        "print(\"Search (AND) - 'space mission':\", search(\"space mission\", inverted_index))\n",
        "print(\"Search (AND) - 'comput mission':\", search(\"comput mission\", inverted_index))\n",
        "\n",
        "# OR Queries\n",
        "print(\"Search (OR) - 'computer science':\", search_or(\"computer science\", inverted_index))\n",
        "print(\"Search (OR) - 'space nasa mission':\", search_or(\"space nasa mission\", inverted_index))"
      ],
      "metadata": {
        "id": "aw8xWi0CH0r4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Stemming – Applied PorterStemmer\n",
        "Add Stemming Using NLTK’s PorterStemmer\n",
        "Here’s exactly what to add to your existing normalize() function"
      ],
      "metadata": {
        "id": "X3yJ0d4UPJX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "# Download required resources\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize stemmer and stopword list\n",
        "stemmer = PorterStemmer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Normalization function\n",
        "def normalize(tokens):\n",
        "    normalized = []\n",
        "    for token in tokens:\n",
        "        token = token.lower().strip(string.punctuation)\n",
        "        if token and token not in stop_words:\n",
        "            stemmed = stemmer.stem(token)\n",
        "            normalized.append(stemmed)\n",
        "    return normalized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZjA3nMyPDwl",
        "outputId": "f9186324-6b20-4fb5-8213-ac2f215c3072"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizer_for_vectorizer(text):\n",
        "    return normalize(tokenize(text))"
      ],
      "metadata": {
        "id": "wVsbb_YTPzUV"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [\"computers\", \"running\", \"happily\", \"drives\", \"mice\"]\n",
        "print(\"After stemming:\", normalize(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yja9R8pPP2up",
        "outputId": "faed5b29-33dd-4a05-afb7-d82ff7312e03"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After stemming: ['comput', 'run', 'happili', 'drive', 'mice']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Vectorization and Cosine Similarity\n",
        "Vectorization converts processed text into numerical form so we can compute similarity between documents. We use TF-IDF to give higher weight to rare but meaningful terms. Cosine similarity helps identify how similar two documents are in terms of their content, regardless of their length.\n"
      ],
      "metadata": {
        "id": "SnaP31fhLowF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Reuse the same documents (we'll only use 5 here for quick testing)\n",
        "sample_docs = [document_dict[f'doc_{i}'] for i in range(5)]\n",
        "\n",
        "# Initialize vectorizer with preprocessing\n",
        "vectorizer = TfidfVectorizer(\n",
        "    tokenizer=tokenize,         # Custom tokenizer\n",
        "    stop_words='english',       # Built-in stopword removal\n",
        "    lowercase=True              # Lowercasing\n",
        ")\n",
        "\n",
        "# Transform documents into TF-IDF matrix\n",
        "tfidf_matrix = vectorizer.fit_transform(sample_docs)\n",
        "\n",
        "# Compute cosine similarity matrix (doc-to-doc)\n",
        "cosine_sim_matrix = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "# Display similarity between doc_0 and others\n",
        "print(\"Cosine Similarity between doc_0 and other docs:\")\n",
        "for i in range(len(sample_docs)):\n",
        "    print(f\"doc_0 vs doc_{i}: {cosine_sim_matrix[0][i]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFvlPihBL5DL",
        "outputId": "773e4b59-d854-45e4-92bb-ae10a71013d0"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity between doc_0 and other docs:\n",
            "doc_0 vs doc_0: 1.0000\n",
            "doc_0 vs doc_1: 0.0124\n",
            "doc_0 vs doc_2: 0.0538\n",
            "doc_0 vs doc_3: 0.0000\n",
            "doc_0 vs doc_4: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"TF-IDF Vocabulary:\", list(vectorizer.vocabulary_.keys()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-FaI-PrOVlc",
        "outputId": "a7ff66df-a7a0-4c5c-e129-87b6a4560dc1"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Vocabulary: ['wondering', 'enlighten', 'car', 'saw', 'day', '2door', 'sports', 'looked', 'late', '60s', 'early', '70s', 'called', 'bricklin', 'doors', 'really', 'small', 'addition', 'bumper', 'separate', 'rest', 'body', 'know', 'tellme', 'model', 'engine', 'specs', 'years', 'production', 'history', 'info', 'funky', 'looking', 'email', 'fair', 'number', 'brave', 'souls', 'upgraded', 'si', 'clock', 'oscillator', 'shared', 'experiences', 'poll', 'send', 'brief', 'message', 'detailing', 'procedure', 'speed', 'attained', 'cpu', 'rated', 'add', 'cards', 'adapters', 'heat', 'sinks', 'hour', 'usage', 'floppy', 'disk', 'functionality', '800', '14', 'm', 'floppies', 'especially', 'requested', 'summarizing', 'days', 'network', 'knowledge', 'base', 'upgrade', 'havent', 'answered', 'thanks', 'folks', 'mac', 'plus', 'finally', 'gave', 'ghost', 'weekend', 'starting', 'life', '512k', 'way', '1985', 'sooo', 'im', 'market', 'new', 'machine', 'bit', 'sooner', 'intended', 'picking', 'powerbook', '160', 'maybe', '180', 'bunch', 'questions', 'hopefully', 'somebody', 'answer', 'does', 'anybody', 'dirt', 'round', 'introductions', 'expected', 'id', 'heard', '185c', 'supposed', 'make', 'appearence', 'summer', 'anymore', 'dont', 'access', 'macleak', 'rumors', 'price', 'drops', 'line', 'like', 'ones', 'duos', 'just', 'went', 'recently', 'whats', 'impression', 'display', 'probably', 'swing', 'got', '80mb', '120', 'feel', 'better', 'yea', 'looks', 'great', 'store', 'wow', 'good', 'solicit', 'opinions', 'people', 'use', 'daytoday', 'worth', 'taking', 'size', 'money', 'hit', 'active', 'realize', 'real', 'subjective', 'question', 'ive', 'played', 'machines', 'computer', 'breifly', 'figured', 'actually', 'uses', 'daily', 'prove', 'helpful', 'hellcats', 'perform', 'advance', 'ill', 'post', 'summary', 'news', 'reading', 'time', 'premium', 'finals', 'corner', 'tom', 'willis', 'twillisecnpurdueedu', 'purdue', 'electrical', 'engineering', 'weiteks', 'addressphone', 'information', 'chip', 'article', 'c5owcbn3pworldstdcom', 'tombakerworldstdcom', 'baker', 'understanding', 'errors', 'basically', 'known', 'bugs', 'warning', 'software', 'things', 'checked', 'right', 'values', 'arent', 'set', 'till', 'launch', 'suchlike', 'fix', 'code', 'possibly', 'introduce', 'tell', 'crew', 'ok', '213', 'liftoff', 'ignore']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query Test (TF-IDF + Cosine):\n",
        "We now test the similarity of a user query like \"space shuttle program\" to each of the documents and rank them."
      ],
      "metadata": {
        "id": "yrQEEuKuMGRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = [\"floppy disk\"]\n",
        "query_vec = vectorizer.transform(query)\n",
        "similarities = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
        "\n",
        "ranked_docs = sorted(zip(range(len(sample_docs)), similarities), key=lambda x: -x[1])\n",
        "\n",
        "print(f\"\\nQuery: '{query[0]}'\")\n",
        "print(\"Ranked documents (doc_id, similarity score):\")\n",
        "for doc_id, score in ranked_docs:\n",
        "    print(f\"doc_{doc_id}: {score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZQvYB_CMQPs",
        "outputId": "24338b06-d4a1-4b1f-c8f9-bd884854cb21"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Query: 'floppy disk'\n",
            "Ranked documents (doc_id, similarity score):\n",
            "doc_1: 0.1669\n",
            "doc_2: 0.0714\n",
            "doc_0: 0.0000\n",
            "doc_3: 0.0000\n",
            "doc_4: 0.0000\n"
          ]
        }
      ]
    }
  ]
}